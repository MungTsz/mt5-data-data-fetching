{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pytz\n",
    "import shutil\n",
    "import pendulum\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import MetaTrader5 as mt5\n",
    "from loguru import logger\n",
    "from cloudpathlib import S3Path\n",
    "from utils.logger import configure_logger\n",
    "from result import Err, Ok, is_ok, is_err\n",
    "from utils.constants import timeframes, PAIRS_WITH_XAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name_str = input(\"What is the name of the config file?\")\n",
    "if config_file_name_str == \"\":\n",
    "    config_file_name_str = \"on_demand_config.yaml\"\n",
    "\n",
    "with open(f\"./config/{config_file_name_str}\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mt5_now_datetime():\n",
    "    mt5_tz = pytz.timezone(\"Israel\")\n",
    "    mt5_now_datetime = datetime.datetime.now(mt5_tz)\n",
    "    return mt5_now_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_str_to_target_timezone_datetime(\n",
    "    target_datetime_str: str, timezone\n",
    ") -> datetime:\n",
    "    target_datetime = datetime.datetime.strptime(\n",
    "        target_datetime_str, \"%Y-%m-%d-%H-%M-%S\"\n",
    "    )\n",
    "    target_datetime = timezone.localize(target_datetime)\n",
    "    return target_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_datetime(config) -> Ok[str | list] | Err[str]:\n",
    "    utc_timezone = pytz.timezone(\"UTC\")\n",
    "    type = config[\"type\"]\n",
    "\n",
    "    try:\n",
    "        if type == \"now\":\n",
    "            now = get_mt5_now_datetime()\n",
    "            now_str = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            mt5_now_datetime = change_str_to_target_timezone_datetime(\n",
    "                now_str, utc_timezone\n",
    "            )\n",
    "            return Ok(mt5_now_datetime)\n",
    "        elif type == \"specific_datetime\":\n",
    "            specific_datetime_str = config[\"specific_datetime\"]\n",
    "            specific_datetime = change_str_to_target_timezone_datetime(\n",
    "                specific_datetime_str, utc_timezone\n",
    "            )\n",
    "            return Ok(specific_datetime)\n",
    "        elif type == \"range\":\n",
    "            datetime_list = []\n",
    "            start_datetime_str = config[\"start_datetime\"]\n",
    "            start_datetime = change_str_to_target_timezone_datetime(\n",
    "                start_datetime_str, utc_timezone\n",
    "            )\n",
    "            datetime_list.append(start_datetime)\n",
    "\n",
    "            end_datetime_str = config[\"end_datetime\"]\n",
    "            end_datetime = change_str_to_target_timezone_datetime(\n",
    "                end_datetime_str, utc_timezone\n",
    "            )\n",
    "            datetime_list.append(end_datetime)\n",
    "            return Ok(datetime_list)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in configuring datetime: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_timeframe(config):\n",
    "    timeframe_list = config[\"timeframe_list\"]\n",
    "    try:\n",
    "        if timeframe_list[0] == \"all\":\n",
    "            timeframe_dict = timeframes\n",
    "            return Ok(timeframe_dict)\n",
    "        elif timeframe_list[0] != \"all\":\n",
    "            timeframe_dict = {key: timeframes[key] for key in timeframe_list}\n",
    "            return Ok(timeframe_dict)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in configuring timeframe: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_pair(config):\n",
    "    pair_list = config[\"pair_list\"]\n",
    "\n",
    "    try:\n",
    "        if pair_list[0] == \"all\":\n",
    "            pair_list = PAIRS_WITH_XAU\n",
    "            return Ok(pair_list)\n",
    "        elif pair_list[0] != \"all\":\n",
    "            # Check if all elements in pair_list are in PAIRS_WITH_XAU list, avoid misspelling\n",
    "            is_all_in_PAIRS_WITH_XAU = all(item in PAIRS_WITH_XAU for item in pair_list)\n",
    "            unmatched_items = [item for item in pair_list if item not in PAIRS_WITH_XAU]\n",
    "            if is_all_in_PAIRS_WITH_XAU is False:\n",
    "                return Err(\n",
    "                    f\"Error in configuring pair, unmatched items found: {unmatched_items}\"\n",
    "                )\n",
    "            return Ok(pair_list)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in configuring pair: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_logger(local_log_path_name: str, logger_name: str):\n",
    "    datetime_now = pendulum.now(\"Asia/Hong_Kong\").strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    configure_logger(local_log_path_name, f\"{logger_name}_{datetime_now}\")\n",
    "    logger.success(\"Logger configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mt5(mt5_file_path: Path):\n",
    "    if mt5.initialize():\n",
    "        return Ok(\"MT5 initialized successfully\")\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Finding the MT5 executable file\")\n",
    "        os.startfile(mt5_file_path)\n",
    "    # if cannot find the file, catch the error msg and return false\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error starting MT5: {e}\")\n",
    "        return Err(\"MT5 initialization failed\")\n",
    "\n",
    "    # if cannot find the file, catch the error msg and return false\n",
    "    if not mt5.initialize():\n",
    "        return Err(\"MT5 initialization failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_range_datetime_data(\n",
    "    symbol: str, timeframe: int, start_datetime: datetime, end_datetime: datetime\n",
    ") -> np.array:\n",
    "    NUMBER_OF_RETRY = 5\n",
    "\n",
    "    for i in range(NUMBER_OF_RETRY):\n",
    "        rates = mt5.copy_rates_range(symbol, timeframe, start_datetime, end_datetime)\n",
    "        if rates is not None:\n",
    "            return Ok(rates)\n",
    "    return Err(\"Error in data fetching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_specific_datetime_data(\n",
    "    symbol: str, timeframe: int, specific_datetime: datetime\n",
    "):\n",
    "    NUMBER_OF_RETRY = 5\n",
    "\n",
    "    for i in range(NUMBER_OF_RETRY):\n",
    "        rates = mt5.copy_rates_from(symbol, timeframe, specific_datetime, 1)\n",
    "        if rates is not None:\n",
    "            return Ok(rates)\n",
    "    return Err(\"Error in data fetching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_datetime(data: pd.DataFrame):\n",
    "    df = data.assign(\n",
    "        time=pd.to_datetime(data[\"time\"], unit=\"s\"),\n",
    "        Date=lambda x: x[\"time\"].dt.date,\n",
    "        Time=lambda x: x[\"time\"].dt.time,\n",
    "    )\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str)\n",
    "    )\n",
    "    df = df.drop([\"time\", \"Date\", \"Time\"], axis=1)\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(\"datetime\")\n",
    "    cols.insert(0, \"datetime\")\n",
    "    df[\"datetime\"] = df[\"datetime\"].dt.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    df = df[cols]\n",
    "    if df is None:\n",
    "        return Err(\"The DataFrame is none\")\n",
    "    return Ok(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_df_to_csv(\n",
    "    df: pd.DataFrame,\n",
    "    pair: str,\n",
    "    timeframe: str,\n",
    "    datetime_str: str,\n",
    "    local_data_base_path: Path,\n",
    "):\n",
    "    output_path = local_data_base_path / pair\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_path / f\"{pair}_{timeframe}_{datetime_str}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    if not output_file.exists():\n",
    "        return Err(f\"{output_file} does not exist\")\n",
    "    return Ok(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_into_rows(df: pd.DataFrame) -> list:\n",
    "    dfs = []\n",
    "    row_count = len(df)\n",
    "    for i in range(row_count):\n",
    "        df_new = df.iloc[i]\n",
    "        df_new = pd.DataFrame(df_new, index=None)\n",
    "        df_new_transpose = df_new.T\n",
    "        dfs.append(df_new_transpose)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_output_range_data_to_csv(\n",
    "    local_data_base_path: Path,\n",
    "    start_datetime: datetime,\n",
    "    end_datetime: datetime,\n",
    "    timeframe_dict: dict,\n",
    "    pair_list: list,\n",
    "):\n",
    "    try:\n",
    "        for pair in pair_list:\n",
    "            for timeframe, timeframe_value in timeframe_dict.items():\n",
    "                fetch_range_datetime_data_result = fetch_range_datetime_data(\n",
    "                    pair, timeframe_value, start_datetime, end_datetime\n",
    "                )\n",
    "                if is_err(fetch_range_datetime_data_result):\n",
    "                    return Err(\n",
    "                        f\"{pair}_{timeframe}: {fetch_range_datetime_data_result.err_value}\"\n",
    "                    )\n",
    "                rates = fetch_range_datetime_data_result.ok_value\n",
    "                unprocessed_df = pd.DataFrame(rates)\n",
    "                logger.info(f\"fetched {pair}_{timeframe}\")\n",
    "                convert_time_to_datetime_result = convert_time_to_datetime(\n",
    "                    unprocessed_df\n",
    "                )\n",
    "                if is_err(convert_time_to_datetime_result):\n",
    "                    return Err(\n",
    "                        f\"{pair}_{timeframe}: {convert_time_to_datetime_result.err_value}\"\n",
    "                    )\n",
    "                temp = convert_time_to_datetime_result.ok_value\n",
    "                logger.info(f\"{pair}_{timeframe} Shape: {temp.shape}\")\n",
    "                dfs = split_df_into_rows(temp)\n",
    "                for df in dfs:\n",
    "                    datetime_str = df[\"datetime\"].iloc[0]\n",
    "                    output_df_to_csv_result = output_df_to_csv(\n",
    "                        df, pair, timeframe, datetime_str, local_data_base_path\n",
    "                    )\n",
    "                    if is_err(output_df_to_csv_result):\n",
    "                        return Err(output_df_to_csv_result.err_value)\n",
    "                    output_file = output_df_to_csv_result.ok_value\n",
    "                    logger.success(f\"{pair}_{timeframe}_{output_file} created\")\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in fetching and output range of data to csv: {e}\")\n",
    "    return Ok(\"Finish fetching and output range of data to csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_output_data_to_csv(\n",
    "    local_data_base_path: Path,\n",
    "    target_datetime: datetime,\n",
    "    timeframe_dict: dict,\n",
    "    pair_list: list,\n",
    "):\n",
    "    try:\n",
    "        for pair in pair_list:\n",
    "            for timeframe, timeframe_value in timeframe_dict.items():\n",
    "                fetch_specific_datetime_data_result = fetch_specific_datetime_data(\n",
    "                    pair, timeframe_value, target_datetime\n",
    "                )\n",
    "                if is_err(fetch_specific_datetime_data_result):\n",
    "                    return Err(\n",
    "                        f\"{pair}_{timeframe}: {fetch_specific_datetime_data_result.err_value}\"\n",
    "                    )\n",
    "                rates = fetch_specific_datetime_data_result.ok_value\n",
    "                unprocessed_df = pd.DataFrame(rates)\n",
    "                logger.info(f\"Fetched {pair}_{timeframe}\")\n",
    "                convert_time_to_datetime_result = convert_time_to_datetime(\n",
    "                    unprocessed_df\n",
    "                )\n",
    "                if is_err(convert_time_to_datetime_result):\n",
    "                    return Err(\n",
    "                        f\"{pair}_{timeframe}: {convert_time_to_datetime_result.err_value}\"\n",
    "                    )\n",
    "                temp = convert_time_to_datetime_result.ok_value\n",
    "                logger.info(f\"{pair}_{timeframe} Shape: {temp.shape}\")\n",
    "                datetime_str = temp[\"datetime\"].iloc[0]\n",
    "                output_df_to_csv_result = output_df_to_csv(\n",
    "                    temp, pair, timeframe, datetime_str, local_data_base_path\n",
    "                )\n",
    "                if is_err(output_df_to_csv_result):\n",
    "                    return Err(output_df_to_csv_result.err_value)\n",
    "                output_file = output_df_to_csv_result.ok_value\n",
    "                logger.success(f\"{pair}_{timeframe}_{output_file} created\")\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in fetching and output data to csv: {e}\")\n",
    "    return Ok(\"Finish fetching and output data to csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key(filename: str):\n",
    "    pair, timeframe, date = filename.split(\"_\")\n",
    "    year, month, day, hour, minute, second = date.split(\"-\")\n",
    "    return f\"{timeframe}/{year}/{month}/{day}/{hour}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data_to_s3(\n",
    "    local_data_base_path: Path, s3_output_base_path: S3Path, pair_list: list\n",
    "):\n",
    "    try:\n",
    "        for pair in pair_list:\n",
    "            local_file_path = local_data_base_path / pair\n",
    "            for path in local_file_path.glob(\"**/*.csv\"):\n",
    "                filename = path.name\n",
    "                s3_key = generate_key(filename)\n",
    "                s3_pair_path = s3_output_base_path / pair / s3_key\n",
    "                s3_pair_path.upload_from(str(path))\n",
    "                logger.success(f\"Successfully uploaded {path} to {s3_pair_path}\")\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error uploading {local_file_path} to {s3_pair_path}: {e}\")\n",
    "    return Ok(\"Finish uploading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_latest_log_to_s3(local_log_path: Path, s3_output_base_path: S3Path):\n",
    "    log_files = list(local_log_path.glob(\"*\"))\n",
    "    # get the latest log file and upload it, avoid upload irrelevant log files\n",
    "    # st_mtime represents the time of the last modification of the file in seconds\n",
    "    latest_log_file = max(log_files, key=lambda f: f.stat().st_mtime)\n",
    "    s3_upload_path = s3_output_base_path / \"log\" / latest_log_file.name\n",
    "    try:\n",
    "        s3_upload_path.upload_from(str(latest_log_file))\n",
    "        return Ok(f\"{latest_log_file} uploaded to {s3_output_base_path}\")\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error uploading {latest_log_file} to {s3_output_base_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_local_data(local_data_base_path: Path):\n",
    "    try:\n",
    "        shutil.rmtree(local_data_base_path)\n",
    "        logger.success(f\"{local_data_base_path} deleted\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting {local_data_base_path}: {e}\")\n",
    "\n",
    "\n",
    "def delete_all_local_data(local_data_base_path, local_log_path):\n",
    "    logger.info(\"Deleting local data\")\n",
    "    delete_local_data(local_data_base_path)\n",
    "    # after delete all data amd catch the log msg, remove the log the upload to S3\n",
    "    logger.remove()\n",
    "    delete_local_data(local_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetching_job(config_file_name_str: str):\n",
    "    # config path\n",
    "    with open(f\"./config/{config_file_name_str}\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    mt5_file_path = Path(config[\"data_fetching_step\"][\"mt5_file_path\"])\n",
    "    local_data_base_path = Path(config[\"data_fetching_step\"][\"local_data_base_path\"])\n",
    "    s3_output_base_path = S3Path(config[\"data_fetching_step\"][\"s3_output_base_path\"])\n",
    "    job_name = config[\"data_fetching_step\"][\"job_name\"]\n",
    "    type = config[\"type\"]\n",
    "\n",
    "    # config datetime\n",
    "    config_datetime_result = config_datetime(config)\n",
    "    if is_err(config_datetime_result):\n",
    "        logger.error(config_datetime_result.err_value)\n",
    "        return Err(config_datetime_result.err_value)\n",
    "\n",
    "    if type == \"now\" or type == \"specific_datetime\":\n",
    "        target_datetime = config_datetime_result.ok_value\n",
    "    elif type == \"range\":\n",
    "        start_datetime, end_datetime = (\n",
    "            config_datetime_result.ok_value[0],\n",
    "            config_datetime_result.ok_value[1],\n",
    "        )\n",
    "    else:\n",
    "        return Err(\n",
    "            \"Configuring the datetime is not possible due to an invalid input type\"\n",
    "        )\n",
    "\n",
    "    # config timeframe\n",
    "    config_timeframe_result = config_timeframe(config)\n",
    "    if is_err(config_timeframe_result):\n",
    "        logger.error(config_timeframe_result.err_value)\n",
    "        return Err(config_timeframe_result.err_value)\n",
    "    timeframe_dict = config_timeframe_result.ok_value\n",
    "\n",
    "    # config pair\n",
    "    config_pair_result = config_pair(config)\n",
    "    if is_err(config_pair_result):\n",
    "        logger.error(config_pair_result.err_value)\n",
    "        return Err(config_pair_result.err_value)\n",
    "    pair_list = config_pair_result.ok_value\n",
    "\n",
    "    # config logger\n",
    "    local_log_path_name = f\"{job_name}_logs\"\n",
    "    local_log_path = Path(local_log_path_name)\n",
    "    create_local_logger(local_log_path_name, job_name)\n",
    "\n",
    "    # info\n",
    "    logger.info(\"The aim of this job: fetch MT5 data\")\n",
    "    logger.info(f\"Configuration file name is {config_file_name_str}\")\n",
    "    logger.info(f\"Local MT5 data path is ./{local_data_base_path}\")\n",
    "    logger.info(f\"Local log path is ./{local_log_path_name}\")\n",
    "    logger.info(f\"The type of the datetime is {type}\")\n",
    "    logger.info(f\"The target timeframe(s) is/are {timeframe_dict.keys()}\")\n",
    "    logger.info(f\"The target pair(s) is/are {pair_list}\")\n",
    "\n",
    "    if type == \"now\" or type == \"specific_datetime\":\n",
    "        logger.info(\n",
    "            f\"The desired target datetime for fetching the MT5 data is {target_datetime}\"\n",
    "        )\n",
    "    elif type == \"range\":\n",
    "        logger.info(\n",
    "            f\"The specified range of datetime for fetching the MT5 data is from {start_datetime} to {end_datetime}.\"\n",
    "        )\n",
    "    else:\n",
    "        return Err(\"Invalid input type\")\n",
    "    logger.info(f\"The S3 destination of the data and log is {s3_output_base_path}\")\n",
    "\n",
    "    # delete all data before data fetching if old data have not been deleted\n",
    "    if local_data_base_path.exists():\n",
    "        logger.info(\"Clear the MT5 data folder before new data comes in\")\n",
    "        delete_local_data(local_data_base_path)\n",
    "\n",
    "    # initialize mt5\n",
    "    logger.info(\"Trying to initialize MT5\")\n",
    "    initialize_mt5_result = initialize_mt5(mt5_file_path)\n",
    "    if is_err(initialize_mt5_result):\n",
    "        logger.error(initialize_mt5_result.err_value)\n",
    "        upload_latest_log_to_s3(local_log_path, s3_output_base_path)\n",
    "        return Err(initialize_mt5_result.err_value)\n",
    "    logger.success(initialize_mt5_result.ok_value)\n",
    "\n",
    "    # Based on job attribute, fetch data, convert time to datetime, and output to csv\n",
    "    logger.info(\"Start fetching MT5 data\")\n",
    "    if type == \"now\" or type == \"specific_datetime\":\n",
    "        result = fetch_output_data_to_csv(\n",
    "            local_data_base_path, target_datetime, timeframe_dict, pair_list\n",
    "        )\n",
    "    elif type == \"range\":\n",
    "        result = fetch_output_range_data_to_csv(\n",
    "            local_data_base_path,\n",
    "            start_datetime,\n",
    "            end_datetime,\n",
    "            timeframe_dict,\n",
    "            pair_list,\n",
    "        )\n",
    "    else:\n",
    "        return Err(\n",
    "            \"Fetching output data to CSV is not possible due to an invalid input type\"\n",
    "        )\n",
    "    if is_err(result):\n",
    "        logger.error(result.err_value)\n",
    "        upload_latest_log_to_s3(local_log_path, s3_output_base_path)\n",
    "        return Err(result.err_value)\n",
    "    logger.success(result.ok_value)\n",
    "\n",
    "    # upload data\n",
    "    logger.info(\"Start uploading fetched data\")\n",
    "    upload_data_to_s3_result = upload_data_to_s3(\n",
    "        local_data_base_path, s3_output_base_path, pair_list\n",
    "    )\n",
    "    if is_err(upload_data_to_s3_result):\n",
    "        logger.error(upload_data_to_s3_result.err_value)\n",
    "        return Err(upload_data_to_s3_result.err_value)\n",
    "    logger.success(upload_data_to_s3_result.ok_value)\n",
    "\n",
    "    # upload log\n",
    "    logger.info(\"Start uploading log\")\n",
    "    upload_latest_log_to_s3_result = upload_latest_log_to_s3(\n",
    "        local_log_path, s3_output_base_path\n",
    "    )\n",
    "    if is_err(upload_latest_log_to_s3_result):\n",
    "        logger.error(upload_latest_log_to_s3_result.err_value)\n",
    "        return Err(upload_latest_log_to_s3_result.err_value)\n",
    "    logger.success(upload_latest_log_to_s3_result.ok_value)\n",
    "\n",
    "    # delete all data\n",
    "    delete_all_local_data(local_data_base_path, local_log_path)\n",
    "    return Ok(f\"Finish data fetching job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fetching_job(config_file_name_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
